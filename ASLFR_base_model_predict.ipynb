{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook makes predictions with the first model of my solution\n",
        "\n",
        "><p><a href=\"https://colab.research.google.com/github/shlomoron/Google---American-Sign-Language-Fingerspelling-Recognition-12th-place-solution/blob/main/ASLFR_base_model_predict.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a>"
      ],
      "metadata": {
        "id": "eheBJR-WHqPd"
      },
      "id": "eheBJR-WHqPd"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "5e16il68WboC"
      },
      "id": "5e16il68WboC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "def set_seeds(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "qUzrgUNFWj9K"
      },
      "id": "qUzrgUNFWj9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "!pip install cached-property\n",
        "from cached_property import cached_property\n",
        "\n",
        "!pip install fastparquet\n",
        "import fastparquet\n",
        "\n",
        "!pip install Levenshtein\n",
        "import Levenshtein as lev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jA5kStlXBah",
        "outputId": "ea79a683-a4fd-4030-cb30-a1dac74dcbfb"
      },
      "id": "1jA5kStlXBah",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/612.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/612.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: cached-property\n",
            "Successfully installed cached-property-1.5.2\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2023.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.23.5)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.7.0 fastparquet-2023.7.0\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/35/04/9ca97b17da457ed294519477da2aad0799c9ba8eebf37761a5ca94c35534/rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data paths and imports\n",
        "[ctc_tpu](https://www.kaggle.com/datasets/shlomoron/ctc-tpu) contains an implementation of CTC for TensorFlow.  \n",
        "[aslfr_light](https://www.kaggle.com/datasets/shlomoron/aslfr-light) contains two needed files from the much larger [competition dataset](https://www.kaggle.com/competitions/asl-fingerspelling/data).  \n",
        "The [TFRecord dataset](https://www.kaggle.com/datasets/shlomoron/aslfr-tfrecords) contains the training data as TFRecords and is imported straight to the TPU.\n"
      ],
      "metadata": {
        "id": "NBn8hGJyIZ4m"
      },
      "id": "NBn8hGJyIZ4m"
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder='/content/input'\n",
        "working_folder='/content'\n",
        "\n",
        "try:\n",
        "  os.mkdir(input_folder)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "ctc_tpu_path = 'gs://kds-56377fe3e755931e5dec0abf56fc91724117fbe7bab3e27b74e37f3e'\n",
        "!gsutil -m cp -r  $ctc_tpu_path $input_folder/ctc-tpu\n",
        "\n",
        "aslfr_light_folder_name = 'asl-fingerspelling'\n",
        "try:\n",
        "  os.mkdir(os.path.join(input_folder, aslfr_light_folder_name))\n",
        "except:\n",
        "  pass\n",
        "aslfr_light_bucket = 'kds-0edb6d00755af6434e157bf0f17ca3102a6722fdba4325c7c11b0e8c'\n",
        "aslfr_light_path = f'gs://{aslfr_light_bucket}'\n",
        "!gsutil -m cp -r $aslfr_light_path $input_folder/asl-fingerspelling\n",
        "shutil.copytree(f'{input_folder}/{aslfr_light_folder_name}/{aslfr_light_bucket}',\n",
        "                f'{input_folder}/{aslfr_light_folder_name}', dirs_exist_ok=True)\n",
        "\n",
        "tfrecords_path = 'gs://kds-4104ecb783277dae764d8a7e543344b63bd14d4a90cc61b85b9c2307'\n",
        "\n",
        "aslfr_MEANs_STDs_folder_name = 'aslfr_MEANs_STDs'\n",
        "try:\n",
        "  os.mkdir(os.path.join(input_folder, aslfr_MEANs_STDs_folder_name))\n",
        "except:\n",
        "  pass\n",
        "aslfr_MEANs_STDs_bucket = 'kds-6aa19d9b7a0929191862354c0121926a98e0c2a5828fe9d275ca75d3'\n",
        "aslfr_MEANs_STDs_path = f'gs://{aslfr_MEANs_STDs_bucket}'\n",
        "!gsutil -m cp -r $aslfr_MEANs_STDs_path $input_folder/aslfr_MEANs_STDs\n",
        "shutil.copytree(f'{input_folder}/{aslfr_MEANs_STDs_folder_name}/{aslfr_MEANs_STDs_bucket}',\n",
        "                f'{input_folder}/{aslfr_MEANs_STDs_folder_name}', dirs_exist_ok=True)\n",
        "\n",
        "aslfr_base_model_path = 'gs://kds-10c4676ea5d76407cd16c1c709456ceaad133ffd1e93cb64350c0c03'\n",
        "!gsutil -m cp -r  $aslfr_base_model_path $input_folder/aslfr-base-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmX4frJLHrwM",
        "outputId": "9d774d72-e3bf-441d-c7b2-932cbff202b5"
      },
      "id": "tmX4frJLHrwM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://kds-56377fe3e755931e5dec0abf56fc91724117fbe7bab3e27b74e37f3e/CTC_TPU.py...\n",
            "/ [1/1 files][ 56.1 KiB/ 56.1 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/56.1 KiB.                                     \n",
            "Copying gs://kds-0edb6d00755af6434e157bf0f17ca3102a6722fdba4325c7c11b0e8c/character_to_prediction_index.json...\n",
            "Copying gs://kds-0edb6d00755af6434e157bf0f17ca3102a6722fdba4325c7c11b0e8c/train.csv...\n",
            "/ [2/2 files][  5.0 MiB/  5.0 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/5.0 MiB.                                      \n",
            "Copying gs://kds-6aa19d9b7a0929191862354c0121926a98e0c2a5828fe9d275ca75d3/MEANs.p...\n",
            "Copying gs://kds-6aa19d9b7a0929191862354c0121926a98e0c2a5828fe9d275ca75d3/STDs.p...\n",
            "/ [2/2 files][  5.1 KiB/  5.1 KiB] 100% Done                                    \n",
            "Operation completed over 2 objects/5.1 KiB.                                      \n",
            "Copying gs://kds-10c4676ea5d76407cd16c1c709456ceaad133ffd1e93cb64350c0c03/model_epoch_299.h5...\n",
            "\\ [1/1 files][ 36.5 MiB/ 36.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/36.5 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define where the output is saved\n",
        "\n",
        "It is best to save Colab output to a mounted Google drive, but giving public notebook access to your drive can be risky, so the default here is saving to the local disk of the colab session. If you have familiarized yourself with the code here and feel at ease with granting such access, you can unmark the second part in this cell and define the appropriate path to a folder in your drive (the folder that you create a path to should exist already- in this example, content/drive/MyDrive/kaggle/ASLFR already exist on my drive)"
      ],
      "metadata": {
        "id": "-WEnYx871lDY"
      },
      "id": "-WEnYx871lDY"
    },
    {
      "cell_type": "code",
      "source": [
        "save_folder = '/content/save'\n",
        "try:\n",
        "  os.mkdir(save_folder)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_folder_name = 'base_model_predict'\n",
        "ASLFR_folder = '/content/drive/MyDrive/kaggle/ASLFR'\n",
        "save_folder = os.path.join(ASLFR_folder, save_folder_name)\n",
        "try:\n",
        "  os.mkdir(save_folder)\n",
        "except:\n",
        "  pass\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se27jvKI1kjB",
        "outputId": "4c05414b-bbe0-4b4f-fb75-a57bbebe0dd1"
      },
      "id": "Se27jvKI1kjB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the ctc loss function"
      ],
      "metadata": {
        "id": "4N6D4BhvKpvE"
      },
      "id": "4N6D4BhvKpvE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ee8ad9a"
      },
      "outputs": [],
      "source": [
        "# copy our file into the working directory (make sure it has .py suffix)\n",
        "copyfile(src = f\"{input_folder}/ctc-tpu/CTC_TPU.py\", dst = f\"{working_folder}//CTC_TPU.py\")\n",
        "\n",
        "# import all our functions\n",
        "from CTC_TPU import classic_ctc_loss"
      ],
      "id": "7ee8ad9a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TPU boilerplate code"
      ],
      "metadata": {
        "id": "PmjWoS83K69m"
      },
      "id": "PmjWoS83K69m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ac1c922",
        "outputId": "b946d102-2f2e-4630-d8e5-20d348b89cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7d4a7d0bc1c0>\n"
          ]
        }
      ],
      "source": [
        "# Configure Strategy. Assume TPU...if not set default for GPU\n",
        "tpu = None\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=None)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print(\"on TPU\")\n",
        "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(strategy)"
      ],
      "id": "0ac1c922"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landmarks indices"
      ],
      "metadata": {
        "id": "IxHa678pLbgw"
      },
      "id": "IxHa678pLbgw"
    },
    {
      "cell_type": "code",
      "source": [
        "NOSE_old=[\n",
        "    1,2,98,327\n",
        "]\n",
        "LNOSE_old = [98]\n",
        "RNOSE_old = [327]\n",
        "LIP_old = [ 0,\n",
        "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
        "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "]\n",
        "LLIP_old = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
        "RLIP_old = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
        "\n",
        "FACE_old = LIP_old+NOSE_old\n",
        "FACE_old.sort()\n",
        "\n",
        "LPOSE_old = [11, 13, 15, 17, 19, 21, 23]\n",
        "RPOSE_old = [12, 14, 16, 18, 20, 22, 24]\n",
        "POSE_old = LPOSE_old + RPOSE_old\n",
        "\n",
        "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_face_{i}' for i in FACE_old] + [f'x_pose_{i}' for i in POSE_old]\n",
        "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_face_{i}' for i in FACE_old] + [f'y_pose_{i}' for i in POSE_old]\n",
        "\n",
        "SEL_COLS = X + Y\n",
        "\n",
        "print('SEL_COLS size:' + str(len(SEL_COLS)))\n",
        "\n",
        "SEL_COLS_x = [x for x in SEL_COLS if 'x' in x]\n",
        "NOSE = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in NOSE_old]\n",
        "LNOSE = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in LNOSE_old]\n",
        "RNOSE = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in RNOSE_old]\n",
        "LIP = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in LIP_old]\n",
        "LLIP = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in LLIP_old]\n",
        "RLIP = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in RLIP_old]\n",
        "FACE = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and int(x.split('_')[-1]) in FACE_old]\n",
        "\n",
        "LPOSE = [i for i, x in enumerate(SEL_COLS_x) if 'pose' in x and int(x.split('_')[-1]) in LPOSE_old]\n",
        "RPOSE = [i for i, x in enumerate(SEL_COLS_x) if 'pose' in x and int(x.split('_')[-1]) in RPOSE_old]\n",
        "POSE = [i for i, x in enumerate(SEL_COLS_x) if 'pose' in x and int(x.split('_')[-1]) in POSE_old]\n",
        "\n",
        "LHAND = [i for i, x in enumerate(SEL_COLS_x) if 'left_hand' in x]\n",
        "RHAND = [i for i, x in enumerate(SEL_COLS_x) if 'right_hand' in x]\n",
        "\n",
        "POINT_LANDMARKS = FACE+RHAND+LHAND+RPOSE+LPOSE\n",
        "\n",
        "norm_point = [i for i, x in enumerate(SEL_COLS_x) if 'face' in x and x.split('_')[-1] == '17'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FulpKI6Lat2",
        "outputId": "cd2bc43f-6560-4dc6-dc92-86493674c065"
      },
      "id": "4FulpKI6Lat2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEL_COLS size:200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config and stuff"
      ],
      "metadata": {
        "id": "O-CPgz1mMwt2"
      },
      "id": "O-CPgz1mMwt2"
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = False\n",
        "\n",
        "ROWS_PER_FRAME = int(len(SEL_COLS)/2)\n",
        "PAD = 0.\n",
        "NUM_NODES = len(POINT_LANDMARKS)\n",
        "CHANNELS = 6*NUM_NODES\n",
        "print(\"Number of landmarks: \" + str(NUM_NODES))\n",
        "print(\"Number of features: \" + str(CHANNELS))\n",
        "pad_token = 'P'\n",
        "pad_token_idx = 59\n",
        "\n",
        "\n",
        "with open (f\"{input_folder}/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    char_to_num = json.load(f)\n",
        "\n",
        "char_to_num[pad_token] = pad_token_idx\n",
        "num_to_char = {j:i for i,j in char_to_num.items()}\n",
        "\n",
        "inpdir = f\"{input_folder}/asl-fingerspelling\"\n",
        "df = pd.read_csv(f'{inpdir}/train.csv')\n",
        "\n",
        "tffiles = df.file_id.map(lambda x: f'{tfrecords_path}/tfds/{x}.tfrecord').unique()\n",
        "\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=list(char_to_num.keys()),\n",
        "        values=list(char_to_num.values()),\n",
        "    ),\n",
        "    default_value=tf.constant(-1),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "MAX_LEN = 380\n",
        "batch_size = 64*2\n",
        "dim = 384\n",
        "val_len = int(0.05 * len(tffiles))\n",
        "cache = False\n",
        "SHUFFLE = -1\n",
        "\n",
        "if DEBUG:\n",
        "  MAX_LEN = 64\n",
        "  batch_size = 16\n",
        "  dim=48\n",
        "  val_len = 1\n",
        "  cache = False\n",
        "  SHUFFLE = 50\n",
        "\n",
        "print(\"Val len: \" + str(val_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh6aK2BTMYvu",
        "outputId": "8972e1f7-3869-4db4-a2a6-f26fa4bb9ca8"
      },
      "id": "Jh6aK2BTMYvu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of landmarks: 100\n",
            "Number of features: 600\n",
            "Val len: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentations functions"
      ],
      "metadata": {
        "id": "mTdjlI5lNU9u"
      },
      "id": "mTdjlI5lNU9u"
    },
    {
      "cell_type": "code",
      "source": [
        "def interp1d_(x, target_len, method='random'):\n",
        "    length = tf.shape(x)[1]\n",
        "    target_len = tf.maximum(1,target_len)\n",
        "    if method == 'random':\n",
        "        if tf.random.uniform(()) < 0.33:\n",
        "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
        "        else:\n",
        "            if tf.random.uniform(()) < 0.5:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
        "            else:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
        "    else:\n",
        "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
        "    return x\n",
        "\n",
        "def flip_lr(x):\n",
        "    x,y,z = tf.unstack(x, axis=-1)\n",
        "    x = 1-x\n",
        "    new_x = tf.stack([x,y,z], -1)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    lhand = tf.gather(new_x, LHAND, axis=0)\n",
        "    rhand = tf.gather(new_x, RHAND, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[...,None], rhand)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[...,None], lhand)\n",
        "    llip = tf.gather(new_x, LLIP, axis=0)\n",
        "    rlip = tf.gather(new_x, RLIP, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[...,None], rlip)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[...,None], llip)\n",
        "    lpose = tf.gather(new_x, LPOSE, axis=0)\n",
        "    rpose = tf.gather(new_x, RPOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[...,None], rpose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[...,None], lpose)\n",
        "    lnose = tf.gather(new_x, LNOSE, axis=0)\n",
        "    rnose = tf.gather(new_x, RNOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[...,None], rnose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[...,None], lnose)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    return new_x\n",
        "\n",
        "def resample(x, rate=(0.8,1.2)):\n",
        "    rate = tf.random.uniform((), rate[0], rate[1])\n",
        "    length = tf.shape(x)[0]\n",
        "    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n",
        "    new_x = interp1d_(x, new_size)\n",
        "    return new_x\n",
        "\n",
        "def spatial_random_affine(xyz,\n",
        "    scale  = (0.8,1.2),\n",
        "    shear = (-0.15,0.15),\n",
        "    shift  = (-0.1,0.1),\n",
        "    degree = (-30,30),\n",
        "):\n",
        "    center = tf.constant([0.5,0.5])\n",
        "    if scale is not None:\n",
        "        scale = tf.random.uniform((),*scale)\n",
        "        xyz = scale*xyz\n",
        "\n",
        "    if shear is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        shear_x = shear_y = tf.random.uniform((),*shear)\n",
        "        if tf.random.uniform(()) < 0.5:\n",
        "            shear_x = 0.\n",
        "        else:\n",
        "            shear_y = 0.\n",
        "        shear_mat = tf.identity([\n",
        "            [1.,shear_x],\n",
        "            [shear_y,1.]\n",
        "        ])\n",
        "        xy = xy @ shear_mat\n",
        "        center = center + [shear_y, shear_x]\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if degree is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        xy -= center\n",
        "        degree = tf.random.uniform((),*degree)\n",
        "        radian = degree/180*np.pi\n",
        "        c = tf.math.cos(radian)\n",
        "        s = tf.math.sin(radian)\n",
        "        rotate_mat = tf.identity([\n",
        "            [c,s],\n",
        "            [-s, c],\n",
        "        ])\n",
        "        xy = xy @ rotate_mat\n",
        "        xy = xy + center\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if shift is not None:\n",
        "        shift = tf.random.uniform((),*shift)\n",
        "        xyz = xyz + shift\n",
        "\n",
        "    return xyz\n",
        "\n",
        "def temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    l = tf.shape(x)[0]\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n",
        "    mask_offset = tf.random.uniform((), 0, l, dtype=tf.int32)\n",
        "    mask_start = mask_offset\n",
        "    mask_end = tf.clip_by_value(mask_offset+mask_size,0,l)\n",
        "    x = tf.tensor_scatter_nd_update(x,tf.range(mask_start, mask_end)[...,None],tf.fill([mask_end - mask_start,ROWS_PER_FRAME,3],mask_value))\n",
        "    if mask_offset+mask_size>l:\n",
        "      mask_start = 0\n",
        "      mask_end = mask_offset+mask_size - l\n",
        "      x = tf.tensor_scatter_nd_update(x,tf.range(mask_start, mask_end)[...,None],tf.fill([mask_end - mask_start,ROWS_PER_FRAME,3],mask_value))\n",
        "    return x\n",
        "\n",
        "def spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    mask_size_x = tf.random.uniform((), *size)\n",
        "    mask_size_y = tf.random.uniform((), *size)\n",
        "    mask_offset_x = tf.random.uniform(())\n",
        "    mask_offset_y = tf.random.uniform(())\n",
        "    mask_x = ((mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size_x)) | ((0<=x[...,0]) & (x[...,0] < mask_offset_x + mask_size_x -1))\n",
        "    mask_y = ((mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size_y)) | ((0<=x[...,1]) & (x[...,1] < mask_offset_y + mask_size_y -1))\n",
        "    mask = mask_x & mask_y\n",
        "    x = tf.where(mask[...,None], mask_value, x)\n",
        "    return x\n",
        "\n",
        "def augment_fn(x, always=False, max_len=None):\n",
        "    if tf.random.uniform(())<0.8 or always:\n",
        "        x = resample(x, (0.5,1.5))\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = flip_lr(x)\n",
        "    if tf.random.uniform(())<0.75 or always:\n",
        "        x = spatial_random_affine(x)\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = temporal_mask(x)\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = spatial_mask(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Oqtue0luNMwJ"
      },
      "id": "Oqtue0luNMwJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "qOFVKmlDN3on"
      },
      "id": "qOFVKmlDN3on"
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_nan_mean(x, axis=0, keepdims=False):\n",
        "    return tf_nan_sum(x, axis=axis, keepdims=keepdims) / tf_nan_count(x, axis=axis, keepdims=keepdims)\n",
        "\n",
        "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
        "    if center is None:\n",
        "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
        "    d = x - center\n",
        "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
        "\n",
        "def tf_nan_sum(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims)\n",
        "\n",
        "def tf_nan_count(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)"
      ],
      "metadata": {
        "id": "lCxTiZjbN5Rp"
      },
      "id": "lCxTiZjbN5Rp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decode"
      ],
      "metadata": {
        "id": "TWiyFlvbnjqx"
      },
      "id": "TWiyFlvbnjqx"
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_tfrec(record_bytes):\n",
        "    schema = {}\n",
        "    schema[\"frames\"] = tf.io.VarLenFeature(dtype=tf.float32)\n",
        "    schema[\"phrase\"] = tf.io.VarLenFeature(dtype=tf.float32)\n",
        "    features = tf.io.parse_single_example(record_bytes, schema)\n",
        "\n",
        "    frames = tf.sparse.to_dense(features[\"frames\"])\n",
        "    frames = tf.transpose(tf.reshape(frames,(-1, 2, int(len(SEL_COLS)/2))),[0, 2, 1])\n",
        "    phrase = tf.cast(tf.sparse.to_dense(features[\"phrase\"]), tf.int32)\n",
        "\n",
        "    out = {}\n",
        "    out['coordinates']  = frames\n",
        "    out['phrase'] = phrase\n",
        "    return out"
      ],
      "metadata": {
        "id": "7za7bTbHPPAH"
      },
      "id": "7za7bTbHPPAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "JiuLp91QpHee"
      },
      "id": "JiuLp91QpHee"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_continue(inputs, point_landmarks, max_len):\n",
        "    if tf.rank(inputs) == 3:\n",
        "        x = inputs[None,...]\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    mean = tf_nan_mean(tf.gather(x, [norm_point], axis=2), axis=[1,2], keepdims=True)\n",
        "    mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
        "    x = tf.gather(x, point_landmarks, axis=2)\n",
        "    std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
        "    x = (x - mean)/std\n",
        "\n",
        "    length = tf.shape(x)[1]\n",
        "\n",
        "    x = x[...,:2]\n",
        "    dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
        "    dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
        "\n",
        "    x = tf.concat([\n",
        "        tf.reshape(x, (-1,length,len(point_landmarks), 2)),\n",
        "        tf.reshape(dx, (-1,length,len(point_landmarks), 2)),\n",
        "        tf.reshape(dx2, (-1,length,len(point_landmarks), 2)),\n",
        "    ], axis = -1)\n",
        "\n",
        "    if max_len is not None and tf.shape(x)[1] > max_len:\n",
        "        x = tf.image.resize(x[0], (max_len, tf.shape(x)[2]))\n",
        "        x = x[None]\n",
        "\n",
        "    x = tf.concat([\n",
        "        tf.reshape(x, (-1,tf.shape(x)[1],6*len(point_landmarks))),\n",
        "    ], axis = -1)\n",
        "    return tf.cast(x, tf.float32)\n",
        "\n",
        "def normalize(x, MEANs, STDs):\n",
        "  x = (x-MEANs)/STDs\n",
        "  return x\n",
        "\n",
        "def remove_nans(x):\n",
        "  x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
        "  return x\n",
        "\n",
        "def preprocess(x, point_landmarks, max_len, MEANs, STDs, augment=False):\n",
        "    coord = x['coordinates']\n",
        "    coord = tf.concat([coord, tf.zeros(( tf.shape(coord)[0],  tf.shape(coord)[1], 1))], axis = -1)\n",
        "    if augment:\n",
        "        coord = augment_fn(coord, max_len=max_len)\n",
        "    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n",
        "    coord = preprocess_continue(coord, point_landmarks, max_len)[0]\n",
        "    coord = normalize(coord, MEANs, STDs)\n",
        "    coord = remove_nans(coord)\n",
        "    return coord, x['phrase']"
      ],
      "metadata": {
        "id": "TpdA_IVfpKuM"
      },
      "id": "TpdA_IVfpKuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter\n",
        "The first model (this) uses filter_by_length. The second model would use filter_by_lev."
      ],
      "metadata": {
        "id": "ZwS8b2mZo-Wo"
      },
      "id": "ZwS8b2mZo-Wo"
    },
    {
      "cell_type": "code",
      "source": [
        "RHAND_IDX = [i for i, x in enumerate(SEL_COLS_x) if 'right' in x]\n",
        "LHAND_IDX = [i for i, x in enumerate(SEL_COLS_x) if 'left' in x]\n",
        "def f1(): return True\n",
        "def f2(): return False\n",
        "def filter_by_length(x):\n",
        "    frames = x['coordinates']\n",
        "    phrase = x['phrase']\n",
        "    rhand_landmarks = tf.gather(frames, RHAND_IDX, axis=1)\n",
        "    lhand_landmarks = tf.gather(frames, LHAND_IDX, axis=1)\n",
        "    r_nonan = tf.math.reduce_sum(tf.cast(~tf.math.is_nan(rhand_landmarks[:, 0, 0]), tf.int64))\n",
        "    l_nonan = tf.math.reduce_sum(tf.cast(~tf.math.is_nan(lhand_landmarks[:, 0, 0]), tf.int64))\n",
        "    no_nan = tf.math.maximum(r_nonan, l_nonan)\n",
        "    return tf.cond(2*tf.shape(phrase)[0]<tf.cast(no_nan, tf.int32), true_fn=f1, false_fn=f2)\n",
        "\n",
        "def filter_by_lev(x, treshold):\n",
        "    lev = x['lev']\n",
        "    return tf.cond(lev>treshold, true_fn=f1, false_fn=f2)"
      ],
      "metadata": {
        "id": "WrR2d4G_pAd6"
      },
      "id": "WrR2d4G_pAd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define sets\n",
        "**val_files_filtered** set checks the loss and Levenshtein distance on the filtered validation set (same distribution as the train set).  \n",
        "**val_files_unfiltered** checks the loss and Levenshtein distance on the validation set, unfiltered (different distribution than the train set, similar distribution to the leaderboard set).  \n",
        "**sub_train_files** checks the Levenshtein distance on a small part of the train set. The Levenshtein distance is the metric, so it is helpful to know how much the train set overfit it compared to the validation set, but it takes a long time to calculate, so we only do it for a small part of the train set."
      ],
      "metadata": {
        "id": "KZf8M8brp1bh"
      },
      "id": "KZf8M8brp1bh"
    },
    {
      "cell_type": "code",
      "source": [
        "val_files_filtered = tffiles[:val_len]\n",
        "val_files_unfiltered = tffiles[:val_len]\n",
        "\n",
        "if DEBUG:\n",
        "  train_files = tffiles[val_len:val_len+1]\n",
        "  sub_train_files = tffiles[val_len:val_len+1]\n",
        "else:\n",
        "  train_files = tffiles[val_len:]\n",
        "  sub_train_files = tffiles[val_len:val_len+3]"
      ],
      "metadata": {
        "id": "FTuB-AuYp-03"
      },
      "id": "FTuB-AuYp-03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load MEANs and STDs"
      ],
      "metadata": {
        "id": "OVpCMRi0eoaI"
      },
      "id": "OVpCMRi0eoaI"
    },
    {
      "cell_type": "code",
      "source": [
        "MEANs = pickle.load(open(f\"{input_folder}/aslfr_MEANs_STDs/MEANs.p\", \"rb\"))\n",
        "STDs = pickle.load(open(f\"{input_folder}/aslfr_MEANs_STDs/STDs.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "rCrNRsln9cMR"
      },
      "id": "rCrNRsln9cMR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get dataset function"
      ],
      "metadata": {
        "id": "TpZC14Dutkh3"
      },
      "id": "TpZC14Dutkh3"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tfrec_dataset(tfrecords, MEANs = MEANs, STDs = STDs, point_landmarks = POINT_LANDMARKS, batch_size=64, max_len=64, drop_remainder=False,\n",
        "                      augment=False, shuffle=False,to_filter = False, cache = False):\n",
        "    # Initialize dataset with TFRecords\n",
        "    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type = 'GZIP').prefetch(tf.data.AUTOTUNE)\n",
        "    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "\n",
        "    if to_filter:\n",
        "        ds = ds.filter(filter_by_length)\n",
        "        #ds = ds.filter(lambda x: filter_by_lev(x, 0.22))\n",
        "\n",
        "    if DEBUG:\n",
        "        ds = ds.take(64)\n",
        "\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "    if shuffle:\n",
        "        if shuffle == -1:\n",
        "            samples_num = ds.reduce(0, lambda x,_: x+1).numpy()\n",
        "            ds = ds.shuffle(samples_num, reshuffle_each_iteration = True)\n",
        "        else:\n",
        "            ds = ds.shuffle(shuffle, reshuffle_each_iteration = True)\n",
        "\n",
        "    ds = ds.map(lambda x: preprocess(x, point_landmarks, max_len, MEANs, STDs, augment=augment), tf.data.AUTOTUNE)\n",
        "\n",
        "    if batch_size:\n",
        "        ds = ds.padded_batch(batch_size, padding_values=(PAD, pad_token_idx), padded_shapes=([max_len,CHANNELS],[64]), drop_remainder=drop_remainder)\n",
        "\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle == -1:\n",
        "      return ds, samples_num\n",
        "    return ds"
      ],
      "metadata": {
        "id": "aCSYXOxYto2h"
      },
      "id": "aCSYXOxYto2h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get datasets"
      ],
      "metadata": {
        "id": "sj1k6e7qVBjh"
      },
      "id": "sj1k6e7qVBjh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ca9c10",
        "outputId": "072ed396-9481-414c-b574-fae0e1ce9a1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 380, 600]), TensorShape([128, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "val_dataset_filtered = get_tfrec_dataset(val_files_filtered, batch_size=batch_size, max_len=MAX_LEN, drop_remainder=True,\n",
        "                                        shuffle=False, to_filter = True, cache = cache)\n",
        "\n",
        "INPUT_SHAPE = [MAX_LEN, CHANNELS]\n",
        "batch = next(iter(val_dataset_filtered))\n",
        "batch[0].shape, batch[1].shape"
      ],
      "id": "56ca9c10"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model layers"
      ],
      "metadata": {
        "id": "l7LXor2TVFFI"
      },
      "id": "l7LXor2TVFFI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd973a07"
      },
      "outputs": [],
      "source": [
        "#Copied from previous comp 1st place model: https://www.kaggle.com/code/hoyso48/1st-place-solution-training\n",
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:,None,:]\n",
        "        return inputs * nn\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer='glorot_uniform',\n",
        "        name='', **kwargs):\n",
        "        super().__init__(name=name,**kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "                            kernel_size,\n",
        "                            strides=1,\n",
        "                            dilation_rate=dilation_rate,\n",
        "                            padding='valid',\n",
        "                            use_bias=use_bias,\n",
        "                            depthwise_initializer=depthwise_initializer,\n",
        "                            name=name + '_dwconv')\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "def Conv1DBlock(channel_size,\n",
        "          kernel_size,\n",
        "          dilation_rate=1,\n",
        "          drop_rate=0.0,\n",
        "          expand_ratio=2,\n",
        "          se_ratio=0.25,\n",
        "          activation='swish',\n",
        "          name=None):\n",
        "    '''\n",
        "    efficient conv1d block, @hoyso48\n",
        "    '''\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand,\n",
        "            use_bias=True,\n",
        "            activation=activation,\n",
        "            name=name + '_expand_conv')(inputs)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = CausalDWConv1D(kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            use_bias=False,\n",
        "            name=name + '_dwconv')(x)\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
        "\n",
        "        x  = ECA()(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channel_size,\n",
        "            use_bias=True,\n",
        "            name=name + '_project_conv')(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
        "\n",
        "        if (channels_in == channel_size):\n",
        "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def TransformerBlock(dim=256, num_heads=6, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "    return apply\n",
        "\n",
        "def positional_encoding(maxlen, num_hid):\n",
        "        depth = num_hid/2\n",
        "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
        "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
        "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
        "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
        "        pos_encoding = tf.concat(\n",
        "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
        "          axis=-1)\n",
        "        return pos_encoding\n",
        "\n",
        "class LateDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.rate = rate\n",
        "        self.start_step = start_step\n",
        "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
        "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
        "        if training:\n",
        "            self._train_counter.assign_add(1)\n",
        "        return x"
      ],
      "id": "dd973a07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57fec44d"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(labels, logits):\n",
        "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
        "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
        "\n",
        "    loss = classic_ctc_loss(\n",
        "            labels=labels,\n",
        "            logits=logits,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            blank_index=pad_token_idx,\n",
        "        )\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "id": "57fec44d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f772936",
        "outputId": "2b0f6bb4-4bff-471d-8d2f-34612d78d952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim: 384\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 380, 600)]   0           []                               \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, 380, 600)     0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " stem_conv (Dense)              (None, 380, 384)     230400      ['masking[0][0]']                \n",
            "                                                                                                  \n",
            " stem_bn (BatchNormalization)   (None, 380, 384)     1536        ['stem_conv[0][0]']              \n",
            "                                                                                                  \n",
            " 1_expand_conv (Dense)          (None, 380, 768)     295680      ['stem_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 1_dwconv (CausalDWConv1D)      (None, 380, 768)     8448        ['1_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 1_bn (BatchNormalization)      (None, 380, 768)     3072        ['1_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca (ECA)                      (None, 380, 768)     5           ['1_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 1_project_conv (Dense)         (None, 380, 384)     295296      ['eca[0][0]']                    \n",
            "                                                                                                  \n",
            " 1_drop (Dropout)               (None, 380, 384)     0           ['1_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 1_add (Add)                    (None, 380, 384)     0           ['1_drop[0][0]',                 \n",
            "                                                                  'stem_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 2_expand_conv (Dense)          (None, 380, 768)     295680      ['1_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 2_dwconv (CausalDWConv1D)      (None, 380, 768)     5376        ['2_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 2_bn (BatchNormalization)      (None, 380, 768)     3072        ['2_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_1 (ECA)                    (None, 380, 768)     5           ['2_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 2_project_conv (Dense)         (None, 380, 384)     295296      ['eca_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 2_drop (Dropout)               (None, 380, 384)     0           ['2_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 2_add (Add)                    (None, 380, 384)     0           ['2_drop[0][0]',                 \n",
            "                                                                  '1_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_expand_conv (Dense)          (None, 380, 768)     295680      ['2_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_dwconv (CausalDWConv1D)      (None, 380, 768)     2304        ['3_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 3_bn (BatchNormalization)      (None, 380, 768)     3072        ['3_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_2 (ECA)                    (None, 380, 768)     5           ['3_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 3_project_conv (Dense)         (None, 380, 384)     295296      ['eca_2[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_drop (Dropout)               (None, 380, 384)     0           ['3_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 3_add (Add)                    (None, 380, 384)     0           ['3_drop[0][0]',                 \n",
            "                                                                  '2_add[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 380, 384)    0           ['3_add[0][0]']                  \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 380, 384)    768         ['tf.__operators__.add[0][0]']   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_self_attention (Mul  (None, 380, 384)    589824      ['layer_normalization[0][0]']    \n",
            " tiHeadSelfAttention)                                                                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 380, 384)     0           ['multi_head_self_attention[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 380, 384)     0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 380, 384)    768         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 380, 768)     294912      ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 380, 384)     294912      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 380, 384)     0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 380, 384)     0           ['add[0][0]',                    \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " 4_expand_conv (Dense)          (None, 380, 768)     295680      ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 4_dwconv (CausalDWConv1D)      (None, 380, 768)     8448        ['4_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 4_bn (BatchNormalization)      (None, 380, 768)     3072        ['4_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_3 (ECA)                    (None, 380, 768)     5           ['4_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 4_project_conv (Dense)         (None, 380, 384)     295296      ['eca_3[0][0]']                  \n",
            "                                                                                                  \n",
            " 4_drop (Dropout)               (None, 380, 384)     0           ['4_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 4_add (Add)                    (None, 380, 384)     0           ['4_drop[0][0]',                 \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_expand_conv (Dense)          (None, 380, 768)     295680      ['4_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_dwconv (CausalDWConv1D)      (None, 380, 768)     5376        ['5_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 5_bn (BatchNormalization)      (None, 380, 768)     3072        ['5_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_4 (ECA)                    (None, 380, 768)     5           ['5_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 5_project_conv (Dense)         (None, 380, 384)     295296      ['eca_4[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_drop (Dropout)               (None, 380, 384)     0           ['5_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 5_add (Add)                    (None, 380, 384)     0           ['5_drop[0][0]',                 \n",
            "                                                                  '4_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_expand_conv (Dense)          (None, 380, 768)     295680      ['5_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_dwconv (CausalDWConv1D)      (None, 380, 768)     2304        ['6_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 6_bn (BatchNormalization)      (None, 380, 768)     3072        ['6_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_5 (ECA)                    (None, 380, 768)     5           ['6_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 6_project_conv (Dense)         (None, 380, 384)     295296      ['eca_5[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_drop (Dropout)               (None, 380, 384)     0           ['6_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 6_add (Add)                    (None, 380, 384)     0           ['6_drop[0][0]',                 \n",
            "                                                                  '5_add[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 380, 384)    768         ['6_add[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_self_attention_1 (M  (None, 380, 384)    589824      ['layer_normalization_2[0][0]']  \n",
            " ultiHeadSelfAttention)                                                                           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 380, 384)     0           ['multi_head_self_attention_1[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 380, 384)     0           ['6_add[0][0]',                  \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 380, 384)    768         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 380, 768)     294912      ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 380, 384)     294912      ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 380, 384)     0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 380, 384)     0           ['add_2[0][0]',                  \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " 7_expand_conv (Dense)          (None, 380, 768)     295680      ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " 7_dwconv (CausalDWConv1D)      (None, 380, 768)     8448        ['7_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 7_bn (BatchNormalization)      (None, 380, 768)     3072        ['7_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_6 (ECA)                    (None, 380, 768)     5           ['7_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 7_project_conv (Dense)         (None, 380, 384)     295296      ['eca_6[0][0]']                  \n",
            "                                                                                                  \n",
            " 7_drop (Dropout)               (None, 380, 384)     0           ['7_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 7_add (Add)                    (None, 380, 384)     0           ['7_drop[0][0]',                 \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " 8_expand_conv (Dense)          (None, 380, 768)     295680      ['7_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 8_dwconv (CausalDWConv1D)      (None, 380, 768)     5376        ['8_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 8_bn (BatchNormalization)      (None, 380, 768)     3072        ['8_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_7 (ECA)                    (None, 380, 768)     5           ['8_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 8_project_conv (Dense)         (None, 380, 384)     295296      ['eca_7[0][0]']                  \n",
            "                                                                                                  \n",
            " 8_drop (Dropout)               (None, 380, 384)     0           ['8_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 8_add (Add)                    (None, 380, 384)     0           ['8_drop[0][0]',                 \n",
            "                                                                  '7_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 9_expand_conv (Dense)          (None, 380, 768)     295680      ['8_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 9_dwconv (CausalDWConv1D)      (None, 380, 768)     2304        ['9_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 9_bn (BatchNormalization)      (None, 380, 768)     3072        ['9_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_8 (ECA)                    (None, 380, 768)     5           ['9_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 9_project_conv (Dense)         (None, 380, 384)     295296      ['eca_8[0][0]']                  \n",
            "                                                                                                  \n",
            " 9_drop (Dropout)               (None, 380, 384)     0           ['9_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 9_add (Add)                    (None, 380, 384)     0           ['9_drop[0][0]',                 \n",
            "                                                                  '8_add[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 380, 384)    768         ['9_add[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_self_attention_2 (M  (None, 380, 384)    589824      ['layer_normalization_4[0][0]']  \n",
            " ultiHeadSelfAttention)                                                                           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 380, 384)     0           ['multi_head_self_attention_2[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 380, 384)     0           ['9_add[0][0]',                  \n",
            "                                                                  'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 380, 384)    768         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 380, 768)     294912      ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 380, 384)     294912      ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 380, 384)     0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 380, 384)     0           ['add_4[0][0]',                  \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " top_conv (Dense)               (None, 380, 768)     295680      ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " late_dropout (LateDropout)     (None, 380, 768)     1           ['top_conv[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 380, 60)      46140       ['late_dropout[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,512,170\n",
            "Trainable params: 9,497,577\n",
            "Non-trainable params: 14,593\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_model(dim = 384, dropout_step=0):\n",
        "    with strategy.scope():\n",
        "        inp = tf.keras.Input(INPUT_SHAPE)\n",
        "        x = inp\n",
        "\n",
        "        x = tf.keras.layers.Masking(mask_value=0.0)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
        "\n",
        "        x = Conv1DBlock(dim,11,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,7,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,3,drop_rate=0.2)(x)\n",
        "\n",
        "        x = x + positional_encoding(INPUT_SHAPE[0], dim)\n",
        "\n",
        "        x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "        x = Conv1DBlock(dim,11,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,7,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,3,drop_rate=0.2)(x)\n",
        "        x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "        x = Conv1DBlock(dim,11,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,7,drop_rate=0.2)(x)\n",
        "        x = Conv1DBlock(dim,3,drop_rate=0.2)(x)\n",
        "        x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(dim*2,activation='relu',name='top_conv')(x)\n",
        "        x = LateDropout(0.4, start_step=dropout_step)(x)\n",
        "        x = tf.keras.layers.Dense(len(char_to_num))(x)\n",
        "\n",
        "        model = tf.keras.Model(inp, x)\n",
        "\n",
        "        loss = CTCLoss\n",
        "\n",
        "        # Adam Optimizer\n",
        "        optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4.0)\n",
        "        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
        "\n",
        "        model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "        return model\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('dim: ' + str(dim))\n",
        "\n",
        "model = get_model(dim = dim)\n",
        "model.load_weights(f\"{input_folder}/aslfr-base-model/model_epoch_299.h5\")\n",
        "model(batch[0])\n",
        "model.summary()"
      ],
      "id": "2f772936"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "756d47c1"
      },
      "outputs": [],
      "source": [
        "def num_to_char_fn(y):\n",
        "    return [num_to_char.get(x, \"\") for x in y]\n",
        "\n",
        "@tf.function()\n",
        "def decode_phrase(pred):\n",
        "    x = tf.argmax(pred, axis=1)\n",
        "    diff = tf.not_equal(x[:-1], x[1:])\n",
        "    adjacent_indices = tf.where(diff)[:, 0]\n",
        "    x = tf.gather(x, adjacent_indices)\n",
        "    mask = x != pad_token_idx\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    output_text = []\n",
        "    for result in pred:\n",
        "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
        "        output_text.append(result)\n",
        "    return output_text"
      ],
      "id": "756d47c1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions and save"
      ],
      "metadata": {
        "id": "RI-kKN5vEXip"
      },
      "id": "RI-kKN5vEXip"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{input_folder}/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "preds = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(tffiles)):\n",
        "  scores = []\n",
        "\n",
        "  tffile = tffiles[i]\n",
        "  tffile_name = tffile.split('/')[-1].split('.')[0]\n",
        "  print(tffile_name)\n",
        "  dataset_tf = get_tfrec_dataset(tffile, batch_size=batch_size, max_len=MAX_LEN, drop_remainder=False,\n",
        "                                          shuffle=False, to_filter = False, cache = cache)\n",
        "  dataset = [x for x in dataset_tf]\n",
        "  for batch_idx in range(len(dataset)):\n",
        "      preds_batch = model.predict(dataset[batch_idx][0], verbose = 0)\n",
        "      targets_batch = dataset[batch_idx][1]\n",
        "      for pred_idx in range(len(preds_batch)):\n",
        "          pred = \"\".join([rev_character_map.get(s, \"\") for s in decode_phrase(preds_batch[pred_idx]).numpy()])\n",
        "          target = \"\".join([rev_character_map.get(s, \"\") for s in targets_batch[pred_idx].numpy()])\n",
        "          preds.append(pred)\n",
        "          targets.append(target)\n",
        "          scores.append((len(target) - lev.distance(pred, target))/len(target))\n",
        "  pickle.dump(scores, open( f'{save_folder}/{tffile_name}.p', \"wb\" ) )\n",
        "\n",
        "N = [len(phrase) for phrase in targets]\n",
        "lev_dist = [lev.distance(preds[i], targets[i]) for i in range(len(targets))]\n",
        "metric_result = (np.sum(N) - np.sum(lev_dist))/np.sum(N)\n",
        "print(metric_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF4D8e0OEbjn",
        "outputId": "c37320db-765c-4d41-8a58-1f29d5c8103c"
      },
      "id": "sF4D8e0OEbjn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5414471\n",
            "105143404\n",
            "128822441\n",
            "149822653\n",
            "152029243\n",
            "169560558\n",
            "175396851\n",
            "234418913\n",
            "296317215\n",
            "349393104\n",
            "388576474\n",
            "425182931\n",
            "433948159\n",
            "450474571\n",
            "474255203\n",
            "495378749\n",
            "522550314\n",
            "527708222\n",
            "532011803\n",
            "546816846\n",
            "566963657\n",
            "568753759\n",
            "614661748\n",
            "638508439\n",
            "649779897\n",
            "654436541\n",
            "683666742\n",
            "871280215\n",
            "882979387\n",
            "933868835\n",
            "939623093\n",
            "1019715464\n",
            "1021040628\n",
            "1098899348\n",
            "1099408314\n",
            "1133664520\n",
            "1134756332\n",
            "1255240050\n",
            "1320204318\n",
            "1341528257\n",
            "1358493307\n",
            "1365275733\n",
            "1365772051\n",
            "1405046009\n",
            "1448136004\n",
            "1497621680\n",
            "1552432300\n",
            "1557244878\n",
            "1562234637\n",
            "1643479812\n",
            "1647220008\n",
            "1662742697\n",
            "1664666588\n",
            "1726141437\n",
            "1785039512\n",
            "1865557033\n",
            "1880177496\n",
            "1905462118\n",
            "1906357076\n",
            "1920330615\n",
            "1967755728\n",
            "1969985709\n",
            "1997878546\n",
            "2026717426\n",
            "2036580525\n",
            "2072296290\n",
            "2072876091\n",
            "2118949241\n",
            "0.8178067446979561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6PxagQDdhHb"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "id": "j6PxagQDdhHb"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4846.417099,
      "end_time": "2023-08-14T17:07:53.504433",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-08-14T15:47:07.087334",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}